import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


class QwenChatbot:
    def __init__(self, model_name="Qwen/Qwen3-8B"):
        print("åŠ è½½ tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

        print("åŠ è½½ modelï¼ˆå»ºè®®ä½¿ç”¨ RTX 4090ï¼‰...")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",  # è‡ªåŠ¨ä½¿ç”¨ GPU
            torch_dtype=torch.float16,  # ä½¿ç”¨ FP16 åŠ é€Ÿ
            trust_remote_code=True
        ).eval()

        self.history = []

    def generate_response(self, user_input: str):
        # æ„å»ºä¸Šä¸‹æ–‡ + å½“å‰ç”¨æˆ·è¾“å…¥
        self.history.append({"role": "user", "content": user_input})

        # æ„é€  chat prompt
        text = self.tokenizer.apply_chat_template(
            self.history,
            tokenize=False,
            add_generation_prompt=True
        )

        # Tokenize è¾“å…¥å¹¶è½¬åˆ° GPU
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)

        # ä½¿ç”¨æ¨ç†æ¨¡å¼ç”Ÿæˆå“åº”
        with torch.inference_mode():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,  # é™åˆ¶å“åº”é•¿åº¦ï¼Œé˜²å¡æ­»
                do_sample=True,  # é‡‡æ ·æ–¹å¼æ›´è‡ªç„¶
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )

        # è§£ç è¾“å‡ºå†…å®¹ï¼ˆä»…æå–æ–°å¢éƒ¨åˆ†ï¼‰
        generated_ids = outputs[0][inputs.input_ids.shape[1]:]
        response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)

        # è¿½åŠ åˆ°å†å²è®°å½•
        self.history.append({"role": "assistant", "content": response})

        return response

    def reset(self):
        self.history = []


# Example: è¿ç»­å¯¹è¯æ¨¡å¼
if __name__ == "__main__":
    chatbot = QwenChatbot()

    print("\næ¬¢è¿è¿›å…¥é€šä¹‰åƒé—® Qwen3-8B å¯¹è¯æ¨¡å¼ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰\n")

    while True:
        user_input = input("ğŸ§‘ ä½ ï¼š").strip() + ' /no_think'
        if user_input.lower() in ["exit", "quit", "q"]:
            break

        response = chatbot.generate_response(user_input)
        print(f"ğŸ¤– åƒé—®ï¼š{response}\n")
