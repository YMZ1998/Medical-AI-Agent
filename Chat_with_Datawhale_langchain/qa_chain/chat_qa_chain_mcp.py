import re
import os
from datetime import datetime

from Chat_with_Datawhale_langchain.qa_chain.model_to_llm import model_to_llm
from Chat_with_Datawhale_langchain.utils.medical_template import MedicalPromptBuilder
from Chat_with_Datawhale_langchain.utils.template import medical_templates, DEFAULT_TEMPLATE


class Chat_QA_chain_simple:
    def __init__(self, model: str, use_history: bool = False, temperature: float = 0.0, chat_history: list = None,
                 api_key: str = None, template: str = None):
        self.model = model
        self.temperature = temperature
        self.use_history = use_history
        self.chat_history = chat_history or []
        self.api_key = api_key
        self.template = template or DEFAULT_TEMPLATE
        self.dir_path = "chat_logs"

    def clear_history(self):
        self.chat_history.clear()

    def change_history_length(self, history_len: int = 1):
        return self.chat_history[-history_len:]

    def format_prompt(self, question: str) -> str:
        """å°†æ¨¡æ¿å¡«å……ä¸ºæœ€ç»ˆæç¤ºè¯"""
        builder = MedicalPromptBuilder(medical_templates, mode="general")
        if self.use_history:
            builder.set_chat_history(self.chat_history)

        context_prompt = builder.build_prompt(question)
        return context_prompt

    def save_history_to_md(self):
        """ä¿å­˜å¯¹è¯åˆ° markdown æ–‡ä»¶"""
        os.makedirs(self.dir_path, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"chat_history_{timestamp}.md"
        filepath = os.path.join(self.dir_path, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write("# åŒ»ç–—å¯¹è¯è®°å½•\n\n")
            for idx, (q, a) in enumerate(self.chat_history, 1):
                f.write(f"### é—®é¢˜ {idx}ï¼š{q}\n\n")
                f.write(f"**å›ç­”ï¼š** {a}\n\n")

        return filepath

    def answer(self, question: str = None, temperature=None):
        if not question:
            return "", self.chat_history

        # è§¦å‘ MCP ä¿å­˜æŒ‡ä»¤
        if question.strip() in ["ä¿å­˜å¯¹è¯", "ä¿å­˜è®°å½•", "ä¿å­˜èŠå¤©"]:
            file_path = self.save_history_to_md()
            return f"âœ… å¯¹è¯å·²ä¿å­˜è‡³ï¼š`{file_path}`", self.chat_history

        temperature = temperature if temperature is not None else self.temperature
        llm = model_to_llm(self.model, temperature, self.api_key)

        response = llm.invoke(self.format_prompt(question))
        answer = getattr(response, "content", str(response)).strip()
        answer = re.sub(r"\\n", '<br/>', answer)

        self.chat_history.append((question, answer))
        return answer, self.chat_history


if __name__ == '__main__':
    from api_config import api_config

    dashscope_api_key = api_config.get_api_key()

    mode = "general"
    chatbot = Chat_QA_chain_simple(
        model="qwen-turbo",
        use_history=True,
        api_key=dashscope_api_key,
        template=medical_templates.get(mode)
    )

    questions = [
        "ä½ å¥½ï¼Œä½ æœ‰ä»€ä¹ˆæŠ€èƒ½ï¼Ÿï¼Œä¿å­˜å¯¹è¯",
        # "æ€ä¹ˆæé«˜æŠµæŠ—åŠ›ï¼Ÿ",
        # "ä»€ä¹ˆæ˜¯é«˜è¡€å‹çš„åˆæœŸè¡¨ç°ï¼Ÿ",
        "ä¿å­˜å¯¹è¯å•Š"  # ğŸ”¥ è§¦å‘ MCP ä¿å­˜
    ]

    for question in questions:
        print(f"ç”¨æˆ·: {question}")
        answer, history = chatbot.answer(question)
        print(f"åŠ©æ‰‹: {answer}")
        print("-" * 50)
