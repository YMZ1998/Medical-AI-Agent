import gradio as gr
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams


# âœ… åˆå§‹åŒ– tokenizer å’Œæ¨¡å‹ï¼ˆä»…è¿è¡Œä¸€æ¬¡ï¼‰
model_name = "Qwen/Qwen2.5-7B-Instruct-1M"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

llm = LLM(
    model=model_name,
    tensor_parallel_size=1,  # ä½ æœ‰å‡ å¼ æ˜¾å¡å°±è®¾å‡ å¼ ï¼Œå¦‚æœæ˜¯ 4090 å•å¡å°±è®¾ä¸º 1
    max_model_len=32768,
    enable_chunked_prefill=True,
    enforce_eager=True,
)

# âœ… è®¾ç½®æ¨ç†å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05,
    max_tokens=512
)


# âœ… èŠå¤©å‡½æ•°ï¼ˆGradioæ¥å£ï¼‰
def chat(user_input, history):
    # æ„é€ èŠå¤©ä¸Šä¸‹æ–‡ï¼ˆGradioå†å²æ ¼å¼æ˜¯ [[user, assistant], ...]ï¼‰
    messages = [{"role": "system", "content": "You are Qwen, a helpful assistant."}]
    for user, assistant in history:
        messages.append({"role": "user", "content": user})
        messages.append({"role": "assistant", "content": assistant})
    messages.append({"role": "user", "content": user_input})

    # åº”ç”¨QwenèŠå¤©æ¨¡æ¿
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # æ¨¡å‹æ¨ç†
    outputs = llm.generate([prompt], sampling_params)
    output_text = outputs[0].outputs[0].text.strip()

    # è¿”å›æ›´æ–°åçš„å†å²
    history.append([user_input, output_text])
    return history, history


# âœ… æ„å»º Gradio UI ç•Œé¢
with gr.Blocks() as demo:
    gr.Markdown("# ğŸ¤– Qwen2.5 Chatbot with vLLM + Gradio")

    chatbot = gr.Chatbot(label="Qwen2.5 Chatbot").style(height=500)
    user_input = gr.Textbox(label="Your message", placeholder="Type your question here...", lines=1)
    state = gr.State([])

    send_btn = gr.Button("Send")

    send_btn.click(fn=chat, inputs=[user_input, state], outputs=[chatbot, state])
    user_input.submit(fn=chat, inputs=[user_input, state], outputs=[chatbot, state])

# âœ… å¯åŠ¨ Gradio åº”ç”¨
demo.launch(server_name="0.0.0.0", server_port=7860)
